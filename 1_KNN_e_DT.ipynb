{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X_Zenza6v2Vy"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq7qXm0WrxfI"
      },
      "source": [
        "## Atividade 1\n",
        "\n",
        ">**Implemente uma classe que corresponda ao KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fze-MVTJn7-v"
      },
      "outputs": [],
      "source": [
        "\n",
        "class KNN:\n",
        "\n",
        "    def __init__(self, k=3):\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, X_test, distance_metric='l2'):\n",
        "\n",
        "        if distance_metric == 'l1':\n",
        "\n",
        "            dist_func = self._l1_distance\n",
        "\n",
        "        elif distance_metric == 'l2':\n",
        "\n",
        "            dist_func = self._l2_distance\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Métrica de distancia inválida. Escolha 'l1' ou 'l2'.\")\n",
        "\n",
        "        predictions = [self._predict(x, dist_func) for x in X_test]\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x, dist_func):\n",
        "\n",
        "        distances = [dist_func(x, x_train) for x_train in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        return most_common[0][0]\n",
        "\n",
        "    def _l1_distance(self, x1, x2):\n",
        "        return np.sum(np.abs(x1 - x2))\n",
        "\n",
        "    def _l2_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_NtfqytSnF8"
      },
      "source": [
        "## Atividade 2\n",
        "\n",
        "**Agora implemente a classe que corresponde a uma Árvore de decisão**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K9baYoWjn_Du"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "\n",
        "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2):\n",
        "\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # Condições de parada\n",
        "        if depth == self.max_depth or num_samples < self.min_samples_split or num_classes == 1:\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        # Escolha de critério de impureza\n",
        "        if self.criterion == 'gini':\n",
        "            impurity_func = self._gini_impurity\n",
        "        elif self.criterion == 'entropy':\n",
        "            impurity_func = self._entropy_impurity\n",
        "        else:\n",
        "            raise ValueError(\"Critério inválido. Escolha 'gini' ou 'entropy'.\")\n",
        "\n",
        "        #  construa a árvore de decisão recursivamente, dividindo os dados em cada nó com base no recurso e no valor de divisão que resultem na menor impureza.\n",
        "\n",
        "        best_gain = 0\n",
        "        best_feature = None\n",
        "        best_value = None\n",
        "        best_sets = None\n",
        "\n",
        "        impurity_node = impurity_func(y)\n",
        "\n",
        "        for feature in range(num_features):\n",
        "            feature_valores = np.unique(X[:, feature])\n",
        "            for valor in feature_valores:\n",
        "                left_indices = np.where(X[:, feature] < valor)\n",
        "                right_indices = np.where(X[:, feature] >= valor)\n",
        "                left_y = y[left_indices]\n",
        "                right_y = y[right_indices]\n",
        "                gain = self._information_gain(left_y, right_y, impurity_node)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_value = valor\n",
        "                    best_sets = {\n",
        "                        \"left_indices\": left_indices,\n",
        "                        \"right_indices\": right_indices\n",
        "                    }\n",
        "\n",
        "        if best_gain > 0:\n",
        "            left = self._build_tree(\n",
        "                X[best_sets[\"left_indices\"]], y[best_sets[\"left_indices\"]], depth + 1)\n",
        "            right = self._build_tree(\n",
        "                X[best_sets[\"right_indices\"]], y[best_sets[\"right_indices\"]], depth + 1)\n",
        "            return (best_feature, best_value, left, right)\n",
        "        return np.bincount(y).argmax()\n",
        "\n",
        "    def _information_gain(self, left_y, right_y, impurity_node):\n",
        "        p = float(len(left_y)) / (len(left_y) + len(right_y))\n",
        "        return impurity_node - p * self._gini_impurity(left_y) - (1 - p) * self._gini_impurity(right_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        return np.array([self._predict(inputs, self.tree) for inputs in X])\n",
        "\n",
        "    def _predict(self, inputs, tree):\n",
        "        if not isinstance(tree, tuple):\n",
        "            return tree\n",
        "        feature, threshold, left_subtree, right_subtree = tree\n",
        "        if inputs[feature] < threshold:\n",
        "            return self._predict(inputs, left_subtree)\n",
        "        else:\n",
        "            return self._predict(inputs, right_subtree)\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        m = len(y)\n",
        "        return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
        "\n",
        "    def _entropy_impurity(self, y):\n",
        "        m = len(y)\n",
        "        return -sum((np.sum(y == c) / m) * np.log2(np.sum(y == c) / m) for c in np.unique(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia: 100.0%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tree = DecisionTree(criterion='gini', max_depth=3, min_samples_split=2)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Acurácia: {accuracy*100}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKIRNwtKvouY"
      },
      "source": [
        "## **Atividade 3 - Classificação com o dataset Breast cancer**\n",
        "\n",
        "Features: Total de 30\n",
        "\n",
        "Para essa atividade utilizar as features:\n",
        "1. Mean radius\n",
        "2. Mean texture\n",
        "3. Mean perimeter\n",
        "4. Mean area\n",
        "5. Mean smoothness\n",
        "\n",
        "Variável dependente: Presença de câncer de mama maligno ou benigno\n",
        "1. 0:  Indica que o tumor é maligno (câncer de mama maligno).\n",
        "2. 1: Indica que o tumor é benigno (câncer de mama benigno).\n",
        "\n",
        "\n",
        ">**Treine os dados da base breast cancer utilizando o KNN e DT implementados**\n",
        "\n",
        ">**Faça as predições nos dados de teste e compare os dados reais com os preditos**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-0F41MFirLbz",
        "outputId": "683b6dba-d072-492e-9d4e-dd2f59d181d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean radius</th>\n",
              "      <th>Mean texture</th>\n",
              "      <th>Mean perimeter</th>\n",
              "      <th>Mean area</th>\n",
              "      <th>Mean smoothness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Mean radius  Mean texture  Mean perimeter  Mean area  Mean smoothness\n",
              "0          17.99         10.38          122.80     1001.0          0.11840\n",
              "1          20.57         17.77          132.90     1326.0          0.08474\n",
              "2          19.69         21.25          130.00     1203.0          0.10960\n",
              "3          11.42         20.38           77.58      386.1          0.14250\n",
              "4          20.29         14.34          135.10     1297.0          0.10030\n",
              "..           ...           ...             ...        ...              ...\n",
              "564        21.56         22.39          142.00     1479.0          0.11100\n",
              "565        20.13         28.25          131.20     1261.0          0.09780\n",
              "566        16.60         28.08          108.30      858.1          0.08455\n",
              "567        20.60         29.33          140.10     1265.0          0.11780\n",
              "568         7.76         24.54           47.92      181.0          0.05263\n",
              "\n",
              "[569 rows x 5 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = datasets.load_breast_cancer()\n",
        "colunas = ['Mean radius', 'Mean texture',\n",
        "           'Mean perimeter', 'Mean area', 'Mean smoothness']\n",
        "pd.DataFrame(data['data'][:, :5], columns=colunas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN:\n",
            " Acurácia: 90.35%\n",
            " Precisão: 90.54%\n",
            "\n",
            "Árvore de decisão:\n",
            " Acurácia: 90.35%\n",
            " Precisão: 94.12%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data['data'][:, :5]\n",
        "y = data['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Treinando e testando KNN\n",
        "knn = KNN(k=3)\n",
        "knn.fit(X_train, y_train)\n",
        "knn_predictions = knn.predict(X_test, distance_metric='l2')\n",
        "\n",
        "# Treinando e testando Decision Tree\n",
        "tree = DecisionTree(criterion='gini', max_depth=3, min_samples_split=2)\n",
        "tree.fit(X_train, y_train)\n",
        "tree_predictions = tree.predict(X_test)\n",
        "\n",
        "# Avaliando a acurácia\n",
        "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
        "knn_precision = precision_score(y_test, knn_predictions)\n",
        "\n",
        "tree_accuracy = accuracy_score(y_test, tree_predictions)\n",
        "tree_precision = precision_score(y_test, tree_predictions)\n",
        "\n",
        "print(\n",
        "    f'KNN:\\n Acurácia: {knn_accuracy*100:.2f}%\\n Precisão: {knn_precision*100:.2f}%')\n",
        "\n",
        "print(\n",
        "    f'\\nÁrvore de decisão:\\n Acurácia: {tree_accuracy*100:.2f}%\\n Precisão: {tree_precision*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMghafQms3Ve"
      },
      "source": [
        ">**As 5 características do dataset Breast cancer possuem escalas diferentes, o que compromete a taxa de acerto. Crie abaixo novas classes KNN_Norm e DecisionTree_Norm para aplicar um algoritmo de normalização de características, refaça o treinamento com as novas classes e analise o impacto no resultado para os dois classificadores.**\n",
        "\n",
        "Exemplos de algoritmos de normalização que podem ser utilizados:\n",
        "1. Normalização Min-Max (Min-Max normalization): Este método dimensiona os dados para que fiquem dentro de um intervalo específico, geralmente entre 0 e 1.\n",
        "\n",
        "2. Normalização pelo Max: Divide-se cada valor pelo maior valor da amostra. Este método será válido apenas em casos em que os valores forem sempre positivos.\n",
        "\n",
        "3. Normalização Z-Score: Este método transforma os dados para que tenham uma média zero e um desvio padrão de 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Implementando a classe KNN_Norm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yO7MecYw2cW_"
      },
      "outputs": [],
      "source": [
        "class KNN_Norm:\n",
        "\n",
        "    def __init__(self, k=3, norm_method='minmax'):\n",
        "        self.k = k\n",
        "        self.norm_method = norm_method\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = self.normalização(X_train)\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, X_test, distance_metric='l2'):\n",
        "        X_test = self.normalização(X_test)\n",
        "\n",
        "        if distance_metric == 'l1':\n",
        "            dist_func = self._l1_distance\n",
        "        elif distance_metric == 'l2':\n",
        "            dist_func = self._l2_distance\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Métrica de distancia inválida. Escolha 'l1' ou 'l2'.\")\n",
        "\n",
        "        predictions = [self._predict(x, dist_func) for x in X_test]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x, dist_func):\n",
        "        distances = [dist_func(x, x_train) for x_train in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        return most_common[0][0]\n",
        "\n",
        "    def _l1_distance(self, x1, x2):\n",
        "        return np.sum(np.abs(x1 - x2))\n",
        "\n",
        "    def _l2_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "    def normalização(self, X):\n",
        "        if self.norm_method == 'minmax':\n",
        "            return (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "        elif self.norm_method == 'max':\n",
        "            return X / X.max(axis=0)\n",
        "        elif self.norm_method == 'zscore':\n",
        "            return (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Método de normalização inválido. Escolha 'minmax', 'max' ou 'zscore'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Implementando a classe DecisionTree_Norm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecisionTree_Norm:\n",
        "\n",
        "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, norm_method='minmax'):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.norm_method = norm_method\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = self._normalize(X)\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        if depth == self.max_depth or num_samples < self.min_samples_split or num_classes == 1:\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        if self.criterion == 'gini':\n",
        "            impurity_func = self._gini_impurity\n",
        "        elif self.criterion == 'entropy':\n",
        "            impurity_func = self._entropy_impurity\n",
        "        else:\n",
        "            raise ValueError(\"Critério inválido. Escolha 'gini' ou 'entropy'.\")\n",
        "\n",
        "        best_gain = 0\n",
        "        best_feature = None\n",
        "        best_value = None\n",
        "        best_sets = None\n",
        "\n",
        "        impurity_node = impurity_func(y)\n",
        "\n",
        "        for feature in range(num_features):\n",
        "            feature_values = np.unique(X[:, feature])\n",
        "            for value in feature_values:\n",
        "                left_indices = np.where(X[:, feature] < value)[0]\n",
        "                right_indices = np.where(X[:, feature] >= value)[0]\n",
        "                left_y = y[left_indices]\n",
        "                right_y = y[right_indices]\n",
        "                gain = self._information_gain(left_y, right_y, impurity_node)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_value = value\n",
        "                    best_sets = {\n",
        "                        \"left_indices\": left_indices,\n",
        "                        \"right_indices\": right_indices\n",
        "                    }\n",
        "\n",
        "        if best_gain > 0:\n",
        "            left = self._build_tree(\n",
        "                X[best_sets[\"left_indices\"]], y[best_sets[\"left_indices\"]], depth + 1)\n",
        "            right = self._build_tree(\n",
        "                X[best_sets[\"right_indices\"]], y[best_sets[\"right_indices\"]], depth + 1)\n",
        "            return (best_feature, best_value, left, right)\n",
        "        return np.bincount(y).argmax()\n",
        "\n",
        "    def _information_gain(self, left_y, right_y, impurity_node):\n",
        "        p = float(len(left_y)) / (len(left_y) + len(right_y))\n",
        "        return impurity_node - p * self._gini_impurity(left_y) - (1 - p) * self._gini_impurity(right_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = self._normalize(X)\n",
        "        return np.array([self._predict(inputs, self.tree) for inputs in X])\n",
        "\n",
        "    def _predict(self, inputs, tree):\n",
        "        if not isinstance(tree, tuple):\n",
        "            return tree\n",
        "        feature, threshold, left_subtree, right_subtree = tree\n",
        "        if inputs[feature] < threshold:\n",
        "            return self._predict(inputs, left_subtree)\n",
        "        else:\n",
        "            return self._predict(inputs, right_subtree)\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        m = len(y)\n",
        "        return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
        "\n",
        "    def _entropy_impurity(self, y):\n",
        "        m = len(y)\n",
        "        return -sum((np.sum(y == c) / m) * np.log2(np.sum(y == c) / m) for c in np.unique(y))\n",
        "\n",
        "    def _normalize(self, X):\n",
        "        if self.norm_method == 'minmax':\n",
        "            return (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "        elif self.norm_method == 'max':\n",
        "            return X / X.max(axis=0)\n",
        "        elif self.norm_method == 'zscore':\n",
        "            return (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Método de normalização inválido. Escolha 'minmax', 'max' ou 'zscore'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Normalizando e comparando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia KNN sem normalização: 90.35%\n",
            "Acurácia Árvore de Decisão sem normalização: 90.35%\n",
            "\n",
            "Acurácia KNN com minmax normalização: 74.56%\n",
            "Acurácia Árvore de Decisão com minmax normalização: 82.46%\n",
            "\n",
            "Acurácia KNN com max normalização: 57.02%\n",
            "Acurácia Árvore de Decisão com max normalização: 74.56%\n",
            "\n",
            "Acurácia KNN com zscore normalização: 93.86%\n",
            "Acurácia Árvore de Decisão com zscore normalização: 92.11%\n"
          ]
        }
      ],
      "source": [
        "def normalization(norm_method):\n",
        "    \n",
        "    knn_norm = KNN_Norm(k=3, norm_method=norm_method)\n",
        "    knn_norm.fit(X_train, y_train)\n",
        "    y_pred_knn_norm = knn_norm.predict(X_test)\n",
        "    accuracy_knn_norm = accuracy_score(y_test, y_pred_knn_norm)\n",
        "    \n",
        "    tree_norm = DecisionTree_Norm(criterion='gini', max_depth=3, norm_method=norm_method)\n",
        "    tree_norm.fit(X_train, y_train)\n",
        "    y_pred_tree_norm = tree_norm.predict(X_test)\n",
        "    accuracy_tree_norm = accuracy_score(y_test, y_pred_tree_norm)\n",
        "    \n",
        "    return accuracy_knn_norm, accuracy_tree_norm\n",
        "\n",
        "methods = ['minmax', 'max', 'zscore']\n",
        "results = {}\n",
        "for method in methods:\n",
        "    results[method] = normalization(method)\n",
        "\n",
        "print(f\"Acurácia KNN sem normalização: {knn_accuracy*100:.2f}%\")\n",
        "print(f\"Acurácia Árvore de Decisão sem normalização: {tree_accuracy*100:.2f}%\")\n",
        "for method, (acc_knn, acc_tree) in results.items():\n",
        "    print(f\"\\nAcurácia KNN com {method} normalização: {acc_knn*100:.2f}%\")\n",
        "    print(f\"Acurácia Árvore de Decisão com {method} normalização: {acc_tree*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vDm4Icpvfv2"
      },
      "source": [
        ">**Divida o dataset em treinamento, teste e validação. Com o conjunto de validação varie os hiperparâmetros dos classificadores implementados e indique os melhores hiperparâmetros.**\n",
        "\n",
        "Para o KNN indique por exemplo qual o melhor K vizinho (3,5,10..), qual a melhor distância (euclidiana ou manhattan). Para a DT pode variar os parâmetros max_depth, min_samples_split, grau de impureza. Como métricas de avaliação podem utilizar recall,   precision e f-measure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhores hiperparâmetros para KNN: {'k': 10, 'metric': 'l1'}, F1-Score: 0.71\n",
            "Melhores hiperparâmetros para Decision Tree: {'max_depth': 10, 'min_samples_split': 2, 'criterion': 'gini'}, F1-Score: 0.76\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def evaluate_knn(X_train, y_train, X_val, y_val, k_values, distance_metrics):\n",
        "    best_params = None\n",
        "    best_score = 0\n",
        "\n",
        "    for k in k_values:\n",
        "        for metric in distance_metrics:\n",
        "            knn = KNN_Norm(k=k, norm_method='minmax')\n",
        "            knn.fit(X_train, y_train)\n",
        "            y_pred = knn.predict(X_val, distance_metric=metric)\n",
        "\n",
        "            recall = recall_score(y_val, y_pred, average='macro')\n",
        "            precision = precision_score(y_val, y_pred, average='macro')\n",
        "            f1 = f1_score(y_val, y_pred, average='macro')\n",
        "\n",
        "            if f1 > best_score:\n",
        "                best_score = f1\n",
        "                best_params = {'k': k, 'metric': metric}\n",
        "\n",
        "    return best_params, best_score\n",
        "\n",
        "\n",
        "k_values = [3, 5, 10]\n",
        "distance_metrics = ['l1', 'l2']\n",
        "best_knn_params, best_knn_score = evaluate_knn(\n",
        "    X_train, y_train, X_val, y_val, k_values, distance_metrics)\n",
        "\n",
        "print(\n",
        "    f\"Melhores hiperparâmetros para KNN: {best_knn_params}, F1-Score: {best_knn_score:.2f}\")\n",
        "\n",
        "\n",
        "def evaluate_decision_tree(X_train, y_train, X_val, y_val, depths, min_samples, criteria):\n",
        "    best_params = None\n",
        "    best_score = 0\n",
        "\n",
        "    for depth in depths:\n",
        "        for min_samples_split in min_samples:\n",
        "            for criterion in criteria:\n",
        "                tree = DecisionTree_Norm(criterion=criterion, max_depth=depth,\n",
        "                                         min_samples_split=min_samples_split, norm_method='minmax')\n",
        "                tree.fit(X_train, y_train)\n",
        "                y_pred = tree.predict(X_val)\n",
        "\n",
        "                \n",
        "                f1 = f1_score(y_val, y_pred, average='macro')\n",
        "\n",
        "                if f1 > best_score:\n",
        "                    best_score = f1\n",
        "                    best_params = {\n",
        "                        'max_depth': depth, 'min_samples_split': min_samples_split, 'criterion': criterion}\n",
        "\n",
        "    return best_params, best_score\n",
        "\n",
        "\n",
        "depths = [3, 5, 10]\n",
        "min_samples = [2, 5, 10]\n",
        "criteria = ['gini', 'entropy']\n",
        "best_tree_params, best_tree_score = evaluate_decision_tree(\n",
        "    X_train, y_train, X_val, y_val, depths, min_samples, criteria)\n",
        "\n",
        "print(\n",
        "    f\"Melhores hiperparâmetros para Decision Tree: {best_tree_params}, F1-Score: {best_tree_score:.2f}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dDKUaGlwpiw"
      },
      "source": [
        "## **Atividade 4 - Classificação com o dataset Wine**\n",
        "\n",
        "Features: Total de 13\n",
        "\n",
        "Para essa atividade utilizar as features:\n",
        "1. Teor alcoólico\n",
        "2. Ácido málico\n",
        "3. Cinzas\n",
        "\n",
        "Variável dependente: Classe do vinho\n",
        "1. Classe 0: Vinhos da primeira origem geográfica.\n",
        "2. Classe 1: Vinhos da segunda origem geográfica.\n",
        "3. Classe 2: Vinhos da terceira origem geográfica.\n",
        "\n",
        "\n",
        "**Nesta atividade deverá ser implementada uma função que divida os dados em folds para realizar validação cruzada.**\n",
        "\n",
        "**Também deverá ser realizado uma comparação entre um treinamento sem e com a validação cruzada no KNN e DT. Os dados deverão ser divididos em 5 folds. E como métricas de avaliação podem utilizar recall, precision e f-measure**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HtvbE_yoxvko",
        "outputId": "590f1e29-9c1a-4b83-a201-6570eb6b18af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Teor alcoólico</th>\n",
              "      <th>Acidez málica</th>\n",
              "      <th>Cinzas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Teor alcoólico  Acidez málica  Cinzas\n",
              "0             14.23           1.71    2.43\n",
              "1             13.20           1.78    2.14\n",
              "2             13.16           2.36    2.67\n",
              "3             14.37           1.95    2.50\n",
              "4             13.24           2.59    2.87\n",
              "..              ...            ...     ...\n",
              "173           13.71           5.65    2.45\n",
              "174           13.40           3.91    2.48\n",
              "175           13.27           4.28    2.26\n",
              "176           13.17           2.59    2.37\n",
              "177           14.13           4.10    2.74\n",
              "\n",
              "[178 rows x 3 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = datasets.load_wine()\n",
        "colunas = ['Teor alcoólico', 'Acidez málica', 'Cinzas']\n",
        "pd.DataFrame(data['data'][:, :3], columns=colunas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZFHlafRhmYIm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Knn sem validação cruzada: Precision = 89.47%, Recall = 87.70%, F1 = 88.45%\n",
            "Knn com validação cruzada: Precision = 87.80%, Recall = 86.06%, F1 = 86.48%\n",
            "\n",
            "Árvore de Decisão sem validação cruzada: Precision = 89.95%, Recall = 89.95%, F1 = 89.95%\n",
            "Árvore de Decisão com validação cruzada: Precision = 87.50%, Recall = 85.61%, F1 = 86.14%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn import datasets\n",
        "\n",
        "def val_cruz_kfold(X, y, num_folds=5):\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    folds = []\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        folds.append((X_train, y_train, X_test, y_test))\n",
        "    return folds\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data['data'][:, :5]\n",
        "y = data['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Modelos sem validação cruzada\n",
        "knn = KNN(k=3)\n",
        "dt = DecisionTree(criterion='gini', max_depth=3, min_samples_split=2)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "precision_knn = precision_score(y_test, y_pred_knn, average='macro')\n",
        "recall_knn = recall_score(y_test, y_pred_knn, average='macro')\n",
        "f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "precision_dt = precision_score(y_test, y_pred_dt, average='macro')\n",
        "recall_dt = recall_score(y_test, y_pred_dt, average='macro')\n",
        "f1_dt = f1_score(y_test, y_pred_dt, average='macro')\n",
        "\n",
        "# Modelos com validação cruzada\n",
        "folds = val_cruz_kfold(X_train, y_train)\n",
        "\n",
        "precision_knn_cv, recall_knn_cv, f1_knn_cv = [], [], []\n",
        "precision_dt_cv, recall_dt_cv, f1_dt_cv = [], [], []\n",
        "\n",
        "for X_train, y_train, X_test, y_test in folds:\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred_knn = knn.predict(X_test)\n",
        "    precision_knn_cv.append(precision_score(y_test, y_pred_knn, average='macro'))\n",
        "    recall_knn_cv.append(recall_score(y_test, y_pred_knn, average='macro'))\n",
        "    f1_knn_cv.append(f1_score(y_test, y_pred_knn, average='macro'))\n",
        "\n",
        "    dt.fit(X_train, y_train)\n",
        "    y_pred_dt = dt.predict(X_test)\n",
        "    precision_dt_cv.append(precision_score(y_test, y_pred_dt, average='macro'))\n",
        "    recall_dt_cv.append(recall_score(y_test, y_pred_dt, average='macro'))\n",
        "    f1_dt_cv.append(f1_score(y_test, y_pred_dt, average='macro'))\n",
        "\n",
        "print(\n",
        "    f'Knn sem validação cruzada: Precision = {precision_knn*100:.2f}%, Recall = {recall_knn*100:.2f}%, F1 = {f1_knn*100:.2f}%')\n",
        "print(\n",
        "    f'Knn com validação cruzada: Precision = {np.mean(precision_knn_cv)*100:.2f}%, Recall = {np.mean(recall_knn_cv)*100:.2f}%, F1 = {np.mean(f1_knn_cv)*100:.2f}%\\n')\n",
        "\n",
        "print(\n",
        "    f'Árvore de Decisão sem validação cruzada: Precision = {precision_dt*100:.2f}%, Recall = {recall_dt*100:.2f}%, F1 = {f1_dt*100:.2f}%')\n",
        "print(\n",
        "    f'Árvore de Decisão com validação cruzada: Precision = {np.mean(precision_dt_cv)*100:.2f}%, Recall = {np.mean(recall_dt_cv)*100:.2f}%, F1 = {np.mean(f1_dt_cv)*100:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
